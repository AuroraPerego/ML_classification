{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T10:17:08.744141Z",
     "start_time": "2020-01-27T10:17:07.742537Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# mpl.rcParams['figure.figsize'] = (6,6)\n",
    "# mpl.rcParams['figure.dpi'] = 100\n",
    "# mpl.rcParams[\"image.origin\"] = 'lower'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"base_dir\":        \"/eos/home-d/dmapelli/public/latino/\",\n",
    "    \"plot_config\":     \"Full2018v6s5\",\n",
    "    \"cut\":             \"res_sig_mjjincl\",\n",
    "    \"model_version\":   \"v39\",\n",
    "    \"model_tag\":       \"res_4depth_v0\",\n",
    "    \"samples_version\": \"v10\",\n",
    "    \"cols\": ['mjj_vbs', \n",
    "             'vbs_1_pt', \n",
    "             'deltaeta_vbs', \n",
    "             'vjet_1_pt', \n",
    "             'Zlep', \n",
    "         ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(config[\"cols\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.python\n",
    "# config examples\n",
    "\"cut\":             \"boos_sig_mjjincl\",\n",
    "\"cut\":             \"res_sig_mjjincl\",\n",
    "\"model_tag\":       \"res_4depth_v0\",\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T10:38:23.026303Z",
     "start_time": "2020-01-27T10:38:22.620559Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "config_base_dir = os.path.join(config[\"base_dir\"], config[\"plot_config\"])\n",
    "\n",
    "# create the model directory\n",
    "model_dir   = os.path.join(config_base_dir, config[\"cut\"] , \"models\",  config[\"model_version\"])\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# load numpy\n",
    "samples_dir = os.path.join(config_base_dir, config[\"cut\"] , \"samples\", config[\"samples_version\"])\n",
    "import pickle\n",
    "signal = pickle.load(open(os.path.join(samples_dir, \"for_training/signal_balanced.pkl\"),     \"rb\"))\n",
    "bkg    = pickle.load(open(os.path.join(samples_dir, \"for_training/background_balanced.pkl\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples preparation for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using \n",
    "## source /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/setup.sh\n",
    "#!pip3 install --user imbalanced-learn==0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T10:17:29.125979Z",
     "start_time": "2020-01-27T10:17:18.939862Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.keras import balanced_batch_generator\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T10:17:31.734886Z",
     "start_time": "2020-01-27T10:17:31.723341Z"
    }
   },
   "outputs": [],
   "source": [
    "## instead of reading the variables from file,\n",
    "## we now set the variables here, and then dump them\n",
    "# import yaml\n",
    "# yaml_vars = yaml.safe_load(open(os.path.join(model_dir, \"variables.yml\")))\n",
    "# print(\"yaml: \", type(yaml_vars), len(yaml_vars))\n",
    "# cols = yaml_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T10:43:23.689119Z",
     "start_time": "2020-01-27T10:43:23.381391Z"
    }
   },
   "outputs": [],
   "source": [
    "X_sig = signal[config[\"cols\"]].values\n",
    "X_bkg = bkg[config[\"cols\"]].values\n",
    "Y_sig = np.ones(len(X_sig))\n",
    "Y_bkg = np.zeros(len(X_bkg))\n",
    "W_sig = (signal[\"weight_norm\"]).values\n",
    "W_bkg = (bkg[\"weight_norm\"]).values\n",
    "Wnn_sig = (signal[\"weight_\"]).values\n",
    "Wnn_bkg = (bkg[\"weight_\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T10:43:27.283132Z",
     "start_time": "2020-01-27T10:43:27.081356Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.vstack([X_sig, X_bkg])\n",
    "Y = np.hstack([Y_sig, Y_bkg])\n",
    "W = np.hstack([W_sig, W_bkg])\n",
    "Wnn = np.hstack([Wnn_sig, Wnn_bkg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T10:43:32.170798Z",
     "start_time": "2020-01-27T10:43:30.694579Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "pickle.dump(scaler, open(f\"{model_dir}/scaler_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T08:46:36.955475Z",
     "start_time": "2020-01-27T08:46:34.860619Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# i = 2\n",
    "# print(cols[i])\n",
    "# fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,5), dpi=100)\n",
    "# ax1.hist(X[:,i], bins=50)\n",
    "# ax1.set_yscale(\"log\")\n",
    "# ax2.hist(X_scaled[:,i], weights=W, bins=50)\n",
    "# plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T10:17:47.639149Z",
     "start_time": "2020-01-27T10:17:47.039479Z"
    }
   },
   "outputs": [],
   "source": [
    "# _ = plt.hist(W, bins=100)\n",
    "_ = plt.hist(W, bins=100, range=(0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T08:50:38.324736Z",
     "start_time": "2020-01-21T08:50:38.320100Z"
    }
   },
   "source": [
    "##  Balancing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T10:43:36.951718Z",
     "start_time": "2020-01-27T10:43:36.191197Z"
    }
   },
   "outputs": [],
   "source": [
    "config[\"test_size\"] = 0.2\n",
    "#config[\"val_size\"]  = 0.5  ## test != val\n",
    "config[\"val_size\"]  = 0.0  ## test == val\n",
    "\n",
    "X_train, X_temp, y_train, y_temp, W_train, W_temp , Wnn_train, Wnn_temp = train_test_split(X_scaled, Y,  W, Wnn, test_size=config[\"test_size\"])\n",
    "#X_val,   X_test, y_val,   y_test, W_val,   W_test = train_test_split(X_temp,   y_temp, W_temp, test_size=config[\"val_size\"]) ## test != val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T10:43:41.448774Z",
     "start_time": "2020-01-27T10:43:41.444471Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Training   dataset: \", X_train.shape)\n",
    "print(\"Test + Val dataset: \", X_temp.shape)\n",
    "#print(\"Testing    dataset: \", X_test.shape)\n",
    "#print(\"Validation dataset: \", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create generators to balance signal and background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T10:43:47.072964Z",
     "start_time": "2020-01-27T10:43:46.189735Z"
    }
   },
   "outputs": [],
   "source": [
    "config[\"batch_size\"] = 1024\n",
    "\n",
    "training_generator,   steps_per_epoch_train = balanced_batch_generator(X_train, y_train, W_train, batch_size=config[\"batch_size\"], sampler=RandomOverSampler())\n",
    "#validation_generator, steps_per_epoch_val   = balanced_batch_generator(X_val,   y_val,   W_val,   batch_size=config[\"batch_size\"], sampler=RandomOverSampler()) ## test != val\n",
    "validation_generator, steps_per_epoch_val   = balanced_batch_generator(X_temp,  y_temp,  W_temp,   batch_size=config[\"batch_size\"], sampler=RandomOverSampler()) ## test == val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T10:43:12.667494Z",
     "start_time": "2020-01-27T10:43:12.356313Z"
    }
   },
   "outputs": [],
   "source": [
    "# import local module that programmatically returns keras models\n",
    "import dnn_models\n",
    "\n",
    "model = dnn_models.get_model(config[\"model_tag\"], X_train.shape[1])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import and configure the plot loss callback\n",
    "\n",
    "import dnn_plot_loss\n",
    "\n",
    "data = {\n",
    "    \"X_train\": X_train,\n",
    "    #\"X_test\" : X_test, ## test != val\n",
    "    \"X_val\" : X_temp, ## test == val\n",
    "    \"y_train\": y_train,\n",
    "    #\"y_test\" : y_test, ## test != val\n",
    "    \"y_val\" : y_temp, ## test == val\n",
    "    \"W_train\": W_train,\n",
    "    #\"W_test\" : W_test, ## test != val\n",
    "    \"W_val\": W_temp, ## test == val\n",
    "    \"Wnn_train\": Wnn_train,\n",
    "    #\"W_test\" : W_test, ## test != val\n",
    "    \"Wnn_val\": Wnn_temp, ## test == val\n",
    "}\n",
    "\n",
    "plot_losses = dnn_plot_loss.PlotLosses(model, data, batch_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T11:49:32.295591Z",
     "start_time": "2020-01-27T11:13:58.490544Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## TRAINING\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "config[\"epochs\"] = 30\n",
    "\n",
    "history = model.fit_generator(\n",
    "            training_generator, \n",
    "            epochs=config[\"epochs\"],\n",
    "            steps_per_epoch  = steps_per_epoch_train, \n",
    "            validation_data  = validation_generator, \n",
    "            validation_steps = steps_per_epoch_val,\n",
    "            callbacks=[plot_losses],\n",
    "            #callbacks = [], \n",
    "            )\n",
    "\n",
    "config[\"train_time\"] = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"train_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T12:55:58.730850Z",
     "start_time": "2020-01-27T12:55:57.810345Z"
    }
   },
   "outputs": [],
   "source": [
    "## SAVE THE MODEL, ITS METADATA AND TRAINING INFORMATIONS\n",
    "\n",
    "# dump the variables list\n",
    "import yaml\n",
    "varfile = os.path.join(model_dir, \"variables.yml\")\n",
    "if os.path.isfile(varfile):\n",
    "    print(\"ACHTUNG! variables file already existing: old file renamed with '_old'\")\n",
    "    os.rename(varfile, varfile[:-4] + \"_old.yml\")\n",
    "with open(varfile, \"w\") as out_var_file:\n",
    "    out_var_file.write(yaml.dump(config[\"cols\"]))\n",
    "    \n",
    "# dump the config\n",
    "config[\"a__model_dir\"] = model_dir\n",
    "model_config_file = os.path.join(model_dir, \"model_config.yml\")\n",
    "if os.path.isfile(model_config_file):\n",
    "    print(\"ACHTUNG! model_config_file file already existing: old file renamed with '_old'\")\n",
    "    os.rename(model_config_file, model_config_file[:-4] + \"_old.yml\")\n",
    "with open(model_config_file, \"w\") as out_var_file:\n",
    "    out_var_file.write(yaml.dump(config))  \n",
    "\n",
    "# save figure with training summary\n",
    "plot_losses.save_figure( os.path.join(model_dir, \"model_train.png\") )\n",
    "\n",
    "# save keras model\n",
    "model.save( os.path.join(model_dir, \"model.h5\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export:\n",
    "## * keras model to tensorflow model\n",
    "## * tf metadata\n",
    "## * scaler\n",
    "\n",
    "args = {\n",
    "    \"dir\": model_dir,\n",
    "    \"input\": \"model.h5\",\n",
    "    \"output\": \"model.pb\",\n",
    "    \"tf_metadata\": \"tf_metadata.txt\",\n",
    "    \"input_scaler\": \"scaler_model.pkl\",\n",
    "    \"output_scaler\": \"scaler.txt\",\n",
    "}\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "# This line must be executed before loading Keras model.\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model(os.path.join(args[\"dir\"], args[\"input\"]))\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        # Graph -> GraphDef ProtoBuf\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(session, input_graph_def,\n",
    "                                                      output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(),\n",
    "                              output_names=[out.op.name for out in model.outputs])\n",
    "\n",
    "# Save to ./model/tf_model.pb\n",
    "tf.train.write_graph(frozen_graph, args[\"dir\"], args[\"output\"], as_text=False)\n",
    "\n",
    "## save tensorflow metadata\n",
    "with open(os.path.join(args[\"dir\"], args[\"tf_metadata\"]), \"w\") as f:\n",
    "    f.write(str(model.inputs[0].name) + \" \" + str(model.outputs[0].name) + \"\\n\")\n",
    "\n",
    "\n",
    "## Export \n",
    "## * tf tensor input name and output name\n",
    "## * scaler mean_ and scale_ (where scale_ = np.sqrt(var_)) for each variable\n",
    "##\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import yaml\n",
    "yaml_vars = yaml.safe_load(open(os.path.join(args[\"dir\"], \"variables.yml\"), \"r\"))\n",
    "\n",
    "scaler = pickle.load(open(os.path.join(args[\"dir\"], args[\"input_scaler\"]), 'rb'))\n",
    "with open(os.path.join(args[\"dir\"], args[\"output_scaler\"]), \"w\") as f:\n",
    "    for var, mean, scale in zip(yaml_vars, scaler.mean_, scaler.scale_):\n",
    "        f.write(var + \" \" + str(mean) + \" \" + str(scale) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve graphs history\n",
    "## plot \n",
    "# n = 30\n",
    "# plt.hist(plot_losses.dnn_score_plot[n][0][1][:-1], bins=plot_losses.dnn_score_plot[n][0][1], weights=plot_losses.dnn_score_plot[n][0][0], histtype=\"step\")\n",
    "# plt.hist(plot_losses.dnn_score_plot[n][1][1][:-1], bins=plot_losses.dnn_score_plot[n][1][1], weights=plot_losses.dnn_score_plot[n][1][0], histtype=\"step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.python\n",
    "# v34\n",
    "    \"cols\": ['mjj_vbs', \n",
    "             'vbs_0_pt', 'vbs_1_pt', \n",
    "             'deltaeta_vbs', 'deltaphi_vbs', \n",
    "             'mjj_vjet', \n",
    "             'vjet_0_pt', 'vjet_1_pt', \n",
    "             'vjet_0_eta', 'vjet_1_eta', \n",
    "             'Lepton_pt', 'Lepton_eta', 'Lepton_flavour', \n",
    "             'PuppiMET', \n",
    "             'Zvjets_0', 'Zlep', \n",
    "             'Asym_vbs', 'Asym_vjet', 'A_ww', \n",
    "             'Mtw_lep', 'w_lep_pt', 'Mww', 'R_ww', 'R_mw', \n",
    "             'Centr_vbs', 'Centr_ww'\n",
    "         ]\n",
    "\n",
    "# v35\n",
    "    \"cols\": ['mjj_vbs', \n",
    "             'vbs_0_pt', 'vbs_1_pt', \n",
    "             'deltaeta_vbs', 'deltaphi_vbs', \n",
    "             'mjj_vjet', \n",
    "             'vjet_0_pt', 'vjet_1_pt', \n",
    "             'vjet_0_eta', 'vjet_1_eta', \n",
    "             'Lepton_pt', 'Lepton_eta', 'Lepton_flavour', \n",
    "             'PuppiMET', \n",
    "             'Zvjets_0', 'Zlep', \n",
    "             'Asym_vbs', 'Asym_vjet', \n",
    "             'Mww', \n",
    "             'Centr_ww'\n",
    "         ]\n",
    "# v36\n",
    "    \"cols\": ['mjj_vbs', \n",
    "             'vbs_0_pt', 'vbs_1_pt', \n",
    "             'deltaeta_vbs', 'deltaphi_vbs', \n",
    "             'mjj_vjet', \n",
    "             'vjet_0_pt', 'vjet_1_pt', \n",
    "             'vjet_0_eta', 'vjet_1_eta', \n",
    "             'Lepton_pt', 'Lepton_eta', \n",
    "             'PuppiMET', \n",
    "             'Zvjets_0', 'Zlep', \n",
    "             'Asym_vjet', \n",
    "         ]\n",
    "\n",
    "# v37\n",
    "    \"cols\": ['mjj_vbs', \n",
    "             'vbs_0_pt', 'vbs_1_pt', \n",
    "             'deltaeta_vbs', 'deltaphi_vbs', \n",
    "             'mjj_vjet', \n",
    "             'vjet_0_pt', 'vjet_1_pt', \n",
    "             'vjet_1_eta', \n",
    "             'Lepton_eta', \n",
    "             'Zlep', \n",
    "             'Asym_vjet', \n",
    "         ]\n",
    "         \n",
    "# OLD\n",
    "    \"cols\": ['mjj_vbs', 'deltaeta_vbs', \n",
    "            'mjj_vjet', \n",
    "            'Lepton_pt', 'Lepton_eta' ]\n",
    "    \"cols\": ['mjj_vbs', \n",
    "             'vbs_0_pt', 'vbs_1_pt', \n",
    "             'deltaeta_vbs', 'deltaphi_vbs', \n",
    "             'mjj_vjet', \n",
    "             'vjet_0_pt', 'vjet_1_pt', \n",
    "             'vjet_0_eta', 'vjet_1_eta', \n",
    "             'Lepton_pt', 'Lepton_eta', 'Lepton_flavour', \n",
    "         ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
